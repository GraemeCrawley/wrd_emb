{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    WRD_EMB = np.random.uniform(size=(vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    W = np.random.uniform(size=(output_size, input_size))\n",
    "    b = np.random.uniform(size=(output_size, 1))\n",
    "    return W, b\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W, b = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    parameters['b'] = b\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds -- shape: (CBOW_N, number of examples)\n",
    "    \"\"\"\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vecs = np.take(WRD_EMB, inds, axis=0)\n",
    "    word_vecs = word_vecs.reshape(WRD_EMB.shape[1], inds.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs.shape == (WRD_EMB.shape[1], inds.shape[0], inds.shape[1]))\n",
    "    \n",
    "    return word_vecs\n",
    "\n",
    "def sum_(word_vecs):\n",
    "    word_vecs_sum = np.sum(word_vecs, axis=1)\n",
    "    word_vecs_sum = word_vecs_sum.reshape(word_vecs.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs_sum.shape == (word_vecs.shape[0], word_vecs.shape[2]))\n",
    "    \n",
    "    return word_vecs_sum\n",
    "\n",
    "def linear_dense(word_vecs_sum, parameters):\n",
    "    W, b = parameters['W'], parameters['b']\n",
    "    Z = np.dot(W, word_vecs_sum) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], word_vecs_sum.shape[1]))\n",
    "    \n",
    "    return W, b, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0))\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "    \n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vecs = ind_to_word_vecs(inds, parameters)\n",
    "    word_vecs_sum = sum_(word_vecs)\n",
    "    W, b, Z = linear_dense(word_vecs_sum, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vecs'] = word_vecs\n",
    "    caches['word_vecs_sum'] = word_vecs_sum\n",
    "    caches['W'] = W\n",
    "    caches['b'] = b\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out), axis=1), axis=0)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, caches):\n",
    "    Z = caches['Z']\n",
    "    dL_dZ = Z - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == Z.shape)\n",
    "    \n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    W = caches['W']\n",
    "    b = caches['b']\n",
    "    word_vecs_sum = caches['word_vecs_sum']\n",
    "    m = word_vecs_sum.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vecs_sum.T)\n",
    "    dL_db = (1 / m) * np.sum(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dword_vecs_sum = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(b.shape == dL_db.shape)\n",
    "    assert(word_vecs_sum.shape == dL_dword_vecs_sum.shape)\n",
    "    \n",
    "    return dL_dW, dL_db, dL_dword_vecs_sum\n",
    "\n",
    "def sum_backward(dL_dword_vecs_sum, caches):\n",
    "    word_vecs = caches['word_vecs']\n",
    "    CBOW_N = word_vecs.shape[1]\n",
    "    \n",
    "    dL_dword_vecs = (1 / m) * np.ones((dL_dword_vecs_sum.shape[0], CBOW_N)) *\\\n",
    "        np.sum(dL_dword_vecs_sum, axis=1, keepdims=True)\n",
    "\n",
    "    assert((word_vecs.shape[0], word_vecs.shape[1]) == dL_dword_vecs.shape[:2])\n",
    "    \n",
    "    return dL_dword_vecs\n",
    "\n",
    "def backward_propagation(Y, caches):\n",
    "    dL_dz = softmax_backward(Y, caches)\n",
    "    dL_dW, dL_db, dL_dword_vecs_sum = dense_backward(dL_dz, caches)\n",
    "    dL_dword_vecs = sum_backward(dL_dword_vecs_sum, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_db'] = dL_db\n",
    "    gradients['dL_dword_vecs'] = dL_dword_vecs\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    CBOW_N = caches['inds'].shape[0]\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    \n",
    "    inds = caches['inds']\n",
    "    updated_WRD_EMD = parameters['WRD_EMB'][inds.T, :] -\\\n",
    "        learning_rate * gradients['dL_dword_vecs'].T.reshape(1, CBOW_N, -1)\n",
    "    parameters['WRD_EMB'][inds.flatten(), :] = updated_WRD_EMD.reshape(-1, emb_size)\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    parameters['b'] -= learning_rate * gradients['dL_db']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(X, Y, vocab_size, emb_size, learning_rate, epochs, parameters=None, print_cost=False):\n",
    "    costs = []\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        softmax_out, caches = forward_propagation(X, parameters)\n",
    "        cost = cross_entropy(softmax_out, Y)\n",
    "        gradients = backward_propagation(Y, caches)\n",
    "        update_parameters(parameters, caches, gradients, learning_rate)\n",
    "        \n",
    "        costs.append(cost)\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iterations {}: {}\".format(i, np.squeeze(cost)))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data\n",
    "Sentence: I(0) would(1) like(2) to(3) get(4) a(5) better(6) job(7).  \n",
    "vocab_size = 8  \n",
    "```\n",
    "[0, 2] [1]  \n",
    "[1, 3] [2]  \n",
    "[2, 4] [3]  \n",
    "[3, 5] [4]  \n",
    "[4, 6] [5]  \n",
    "[5, 7] [6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = 2\n",
    "vocab_size = 8\n",
    "m = 6\n",
    "emb_size = 15\n",
    "X = np.array([[0, 1, 2, 3, 4, 5],\n",
    "              [2, 3, 4, 5, 6, 7]]) # 2 x 6\n",
    "Y = np.array([1, 2, 3, 4, 5, 6]) # 1 x 6\n",
    "Y_one_hot = np.zeros((vocab_size, m))  # 8 x 6\n",
    "Y_one_hot[Y.flatten(), np.arange(6)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(vocab_size, emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_out, caches = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = cross_entropy(softmax_out, Y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = backward_propagation(Y_one_hot, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iterations 0: 2.4290657988060467\n",
      "Cost after iterations 10000: 1.3812163351456015\n",
      "Cost after iterations 20000: 1.35663956908022\n",
      "Cost after iterations 30000: 1.344882725596848\n",
      "Cost after iterations 40000: 1.3381822336402778\n",
      "Cost after iterations 50000: 1.3337763353864123\n",
      "Cost after iterations 60000: 1.3305434442142525\n",
      "Cost after iterations 70000: 1.3279765663906762\n",
      "Cost after iterations 80000: 1.3258173503540844\n",
      "Cost after iterations 90000: 1.3239222294569237\n",
      "Cost after iterations 100000: 1.322207007992102\n",
      "Cost after iterations 110000: 1.320620596264673\n",
      "Cost after iterations 120000: 1.3191312102651627\n",
      "Cost after iterations 130000: 1.3177186338835392\n",
      "Cost after iterations 140000: 1.3163697090703583\n",
      "Cost after iterations 150000: 1.3150756435269835\n",
      "Cost after iterations 160000: 1.3138303772572137\n",
      "Cost after iterations 170000: 1.3126295796847738\n",
      "Cost after iterations 180000: 1.3114700280534035\n",
      "Cost after iterations 190000: 1.310349219012241\n",
      "Cost after iterations 200000: 1.309265124098826\n",
      "Cost after iterations 210000: 1.3082160346847647\n",
      "Cost after iterations 220000: 1.307200462903149\n",
      "Cost after iterations 230000: 1.306217077814642\n",
      "Cost after iterations 240000: 1.3052646638808678\n",
      "Cost after iterations 250000: 1.304342093639484\n",
      "Cost after iterations 260000: 1.3034483094737577\n",
      "Cost after iterations 270000: 1.3025823112435941\n",
      "Cost after iterations 280000: 1.3017431477207864\n",
      "Cost after iterations 290000: 1.3009299105131542\n",
      "Cost after iterations 300000: 1.3001417296320068\n",
      "Cost after iterations 310000: 1.299377770156171\n",
      "Cost after iterations 320000: 1.2986372296367998\n",
      "Cost after iterations 330000: 1.297919336009964\n",
      "Cost after iterations 340000: 1.297223345863061\n",
      "Cost after iterations 350000: 1.2965485429527235\n",
      "Cost after iterations 360000: 1.2958942369052802\n",
      "Cost after iterations 370000: 1.2952597620531758\n",
      "Cost after iterations 380000: 1.294644476375268\n",
      "Cost after iterations 390000: 1.2940477605187244\n",
      "Cost after iterations 400000: 1.2934690168869667\n",
      "Cost after iterations 410000: 1.2929076687823886\n",
      "Cost after iterations 420000: 1.2923631595958376\n",
      "Cost after iterations 430000: 1.291834952036886\n",
      "Cost after iterations 440000: 1.2913225274003701\n",
      "Cost after iterations 450000: 1.290825384865824\n",
      "Cost after iterations 460000: 1.290343040827048\n",
      "Cost after iterations 470000: 1.2898750282497116\n",
      "Cost after iterations 480000: 1.2894208960552052\n",
      "Cost after iterations 490000: 1.2889802085292312\n",
      "Cost after iterations 500000: 1.2885525447538524\n",
      "Cost after iterations 510000: 1.288137498061925\n",
      "Cost after iterations 520000: 1.2877346755127974\n",
      "Cost after iterations 530000: 1.287343697388497\n",
      "Cost after iterations 540000: 1.2869641967094612\n",
      "Cost after iterations 550000: 1.2865958187689985\n",
      "Cost after iterations 560000: 1.2862382206859029\n",
      "Cost after iterations 570000: 1.2858910709742544\n",
      "Cost after iterations 580000: 1.2855540491299937\n",
      "Cost after iterations 590000: 1.2852268452334465\n",
      "Cost after iterations 600000: 1.2849091595672397\n",
      "Cost after iterations 610000: 1.284600702249068\n",
      "Cost after iterations 620000: 1.2843011928786132\n",
      "Cost after iterations 630000: 1.2840103601981614\n",
      "Cost after iterations 640000: 1.2837279417663796\n",
      "Cost after iterations 650000: 1.2834536836446695\n",
      "Cost after iterations 660000: 1.2831873400956662\n",
      "Cost after iterations 670000: 1.2829286732933716\n",
      "Cost after iterations 680000: 1.2826774530444551\n",
      "Cost after iterations 690000: 1.2824334565203162\n",
      "Cost after iterations 700000: 1.2821964679994091\n",
      "Cost after iterations 710000: 1.2819662786194799\n",
      "Cost after iterations 720000: 1.2817426861392933\n",
      "Cost after iterations 730000: 1.281525494709446\n",
      "Cost after iterations 740000: 1.281314514651943\n",
      "Cost after iterations 750000: 1.2811095622481135\n",
      "Cost after iterations 760000: 1.28091045953462\n",
      "Cost after iterations 770000: 1.2807170341071383\n",
      "Cost after iterations 780000: 1.2805291189314545\n",
      "Cost after iterations 790000: 1.280346552161654\n",
      "Cost after iterations 800000: 1.2801691769651198\n",
      "Cost after iterations 810000: 1.2799968413540768\n",
      "Cost after iterations 820000: 1.2798293980233821\n",
      "Cost after iterations 830000: 1.279666704194311\n",
      "Cost after iterations 840000: 1.2795086214641642\n",
      "Cost after iterations 850000: 1.279355015661347\n",
      "Cost after iterations 860000: 1.279205756705776\n",
      "Cost after iterations 870000: 1.2790607184744227\n",
      "Cost after iterations 880000: 1.2789197786716593\n",
      "Cost after iterations 890000: 1.2787828187043806\n",
      "Cost after iterations 900000: 1.2786497235615923\n",
      "Cost after iterations 910000: 1.278520381698277\n",
      "Cost after iterations 920000: 1.278394684923511\n",
      "Cost after iterations 930000: 1.2782725282924905\n",
      "Cost after iterations 940000: 1.27815381000241\n",
      "Cost after iterations 950000: 1.278038431292\n",
      "Cost after iterations 960000: 1.2779262963446172\n",
      "Cost after iterations 970000: 1.277817312194717\n",
      "Cost after iterations 980000: 1.2777113886376037\n",
      "Cost after iterations 990000: 1.2776084381422779\n"
     ]
    }
   ],
   "source": [
    "parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.005, 1000000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iterations 0: 1.2771419272935438\n",
      "Cost after iterations 10000: 1.2770941164666594\n",
      "Cost after iterations 20000: 1.2770404331081637\n",
      "Cost after iterations 30000: 1.276989081257757\n",
      "Cost after iterations 40000: 1.2769388309767298\n",
      "Cost after iterations 50000: 1.2768895546318006\n",
      "Cost after iterations 60000: 1.2768412134762057\n",
      "Cost after iterations 70000: 1.276793771659594\n",
      "Cost after iterations 80000: 1.2767471946989106\n",
      "Cost after iterations 90000: 1.276701451590308\n",
      "Cost after iterations 100000: 1.2766565152104081\n",
      "Cost after iterations 110000: 1.276612361885264\n",
      "Cost after iterations 120000: 1.2765689707628818\n",
      "Cost after iterations 130000: 1.2765263232265562\n",
      "Cost after iterations 140000: 1.2764844024105098\n",
      "Cost after iterations 150000: 1.2764431928192592\n",
      "Cost after iterations 160000: 1.2764026800346553\n",
      "Cost after iterations 170000: 1.276362850493022\n",
      "Cost after iterations 180000: 1.2763236913163576\n",
      "Cost after iterations 190000: 1.2762851901850338\n",
      "Cost after iterations 200000: 1.2762473352417274\n",
      "Cost after iterations 210000: 1.2762101150190035\n",
      "Cost after iterations 220000: 1.2761735183846084\n",
      "Cost after iterations 230000: 1.2761375345000079\n",
      "Cost after iterations 240000: 1.2761021527888496\n",
      "Cost after iterations 250000: 1.276067362912776\n",
      "Cost after iterations 260000: 1.2760331547527812\n",
      "Cost after iterations 270000: 1.2759995183946105\n",
      "Cost after iterations 280000: 1.2759664441172434\n",
      "Cost after iterations 290000: 1.2759339223836261\n",
      "Cost after iterations 300000: 1.27590194383311\n",
      "Cost after iterations 310000: 1.275870499275121\n",
      "Cost after iterations 320000: 1.275839579683815\n",
      "Cost after iterations 330000: 1.2758091761934232\n",
      "Cost after iterations 340000: 1.2757792800941488\n",
      "Cost after iterations 350000: 1.2757498828284302\n",
      "Cost after iterations 360000: 1.2757209759875674\n",
      "Cost after iterations 370000: 1.275692551308606\n",
      "Cost after iterations 380000: 1.275664600671323\n",
      "Cost after iterations 390000: 1.2756371160954572\n",
      "Cost after iterations 400000: 1.275610089738021\n",
      "Cost after iterations 410000: 1.275583513890724\n",
      "Cost after iterations 420000: 1.2755573809774994\n",
      "Cost after iterations 430000: 1.275531683552056\n",
      "Cost after iterations 440000: 1.2755064142955312\n",
      "Cost after iterations 450000: 1.2754815660142607\n",
      "Cost after iterations 460000: 1.2754571316374614\n",
      "Cost after iterations 470000: 1.2754331042150977\n",
      "Cost after iterations 480000: 1.2754094769157227\n",
      "Cost after iterations 490000: 1.2753862430243919\n",
      "Cost after iterations 500000: 1.2753633959405857\n",
      "Cost after iterations 510000: 1.2753409291762394\n",
      "Cost after iterations 520000: 1.2753188363537156\n",
      "Cost after iterations 530000: 1.2752971112039209\n",
      "Cost after iterations 540000: 1.2752757475643786\n",
      "Cost after iterations 550000: 1.2752547393773503\n",
      "Cost after iterations 560000: 1.2752340806880293\n",
      "Cost after iterations 570000: 1.2752137656427487\n",
      "Cost after iterations 580000: 1.27519378848718\n",
      "Cost after iterations 590000: 1.2751741435646595\n",
      "Cost after iterations 600000: 1.275154825314437\n",
      "Cost after iterations 610000: 1.2751358282700347\n",
      "Cost after iterations 620000: 1.27511714705761\n",
      "Cost after iterations 630000: 1.2750987763943593\n",
      "Cost after iterations 640000: 1.2750807110868863\n",
      "Cost after iterations 650000: 1.2750629460296996\n",
      "Cost after iterations 660000: 1.2750454762037018\n",
      "Cost after iterations 670000: 1.2750282966746411\n",
      "Cost after iterations 680000: 1.2750114025917056\n",
      "Cost after iterations 690000: 1.274994789186028\n",
      "Cost after iterations 700000: 1.2749784517693086\n",
      "Cost after iterations 710000: 1.2749623857324046\n",
      "Cost after iterations 720000: 1.2749465865439724\n",
      "Cost after iterations 730000: 1.2749310497491306\n",
      "Cost after iterations 740000: 1.2749157709681196\n",
      "Cost after iterations 750000: 1.2749007458950676\n",
      "Cost after iterations 760000: 1.274885970296633\n",
      "Cost after iterations 770000: 1.2748714400108645\n",
      "Cost after iterations 780000: 1.274857150945854\n",
      "Cost after iterations 790000: 1.2748430990786297\n",
      "Cost after iterations 800000: 1.2748292804539518\n",
      "Cost after iterations 810000: 1.2748156911831166\n",
      "Cost after iterations 820000: 1.2748023274428504\n",
      "Cost after iterations 830000: 1.2747891854741982\n",
      "Cost after iterations 840000: 1.274776261581359\n",
      "Cost after iterations 850000: 1.274763552130704\n",
      "Cost after iterations 860000: 1.2747510535496165\n",
      "Cost after iterations 870000: 1.2747387623255144\n",
      "Cost after iterations 880000: 1.2747266750048187\n",
      "Cost after iterations 890000: 1.2747147881919008\n",
      "Cost after iterations 900000: 1.2747030985481544\n",
      "Cost after iterations 910000: 1.2746916027909823\n",
      "Cost after iterations 920000: 1.2746802976928635\n",
      "Cost after iterations 930000: 1.2746691800803973\n",
      "Cost after iterations 940000: 1.2746582468334053\n",
      "Cost after iterations 950000: 1.2746474948839728\n",
      "Cost after iterations 960000: 1.2746369212156488\n",
      "Cost after iterations 970000: 1.2746265228625124\n",
      "Cost after iterations 980000: 1.2746162969083117\n",
      "Cost after iterations 990000: 1.2746062404856178\n"
     ]
    }
   ],
   "source": [
    "parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.003, 1000000, parameters=parameters, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iterations 0: 1.2745963507750484\n",
      "Cost after iterations 10000: 1.2745836509003459\n",
      "Cost after iterations 20000: 1.2745707229444228\n",
      "Cost after iterations 30000: 1.2745581899393552\n",
      "Cost after iterations 40000: 1.2745459725070305\n",
      "Cost after iterations 50000: 1.2745340528815519\n",
      "Cost after iterations 60000: 1.274522416475609\n",
      "Cost after iterations 70000: 1.274511050850203\n",
      "Cost after iterations 80000: 1.274499945617501\n",
      "Cost after iterations 90000: 1.2744890919925695\n",
      "Cost after iterations 100000: 1.2744784823582793\n",
      "Cost after iterations 110000: 1.2744681099349349\n",
      "Cost after iterations 120000: 1.274457968546614\n",
      "Cost after iterations 130000: 1.2744480524600938\n",
      "Cost after iterations 140000: 1.2744383562746615\n",
      "Cost after iterations 150000: 1.2744288748470085\n",
      "Cost after iterations 160000: 1.2744196032399444\n",
      "Cost after iterations 170000: 1.274410536687307\n",
      "Cost after iterations 180000: 1.2744016705696382\n",
      "Cost after iterations 190000: 1.2743930003973312\n",
      "Cost after iterations 200000: 1.2743845217985972\n",
      "Cost after iterations 210000: 1.27437623051076\n",
      "Cost after iterations 220000: 1.2743681223738332\n",
      "Cost after iterations 230000: 1.2743601933254585\n",
      "Cost after iterations 240000: 1.2743524393970296\n",
      "Cost after iterations 250000: 1.274344856710344\n",
      "Cost after iterations 260000: 1.2743374414748305\n",
      "Cost after iterations 270000: 1.2743301899850665\n",
      "Cost after iterations 280000: 1.274323098618522\n",
      "Cost after iterations 290000: 1.2743161638334963\n",
      "Cost after iterations 300000: 1.2743093821671343\n",
      "Cost after iterations 310000: 1.2743027502335744\n",
      "Cost after iterations 320000: 1.2742962647221847\n",
      "Cost after iterations 330000: 1.2742899223958235\n",
      "Cost after iterations 340000: 1.2742837200891666\n",
      "Cost after iterations 350000: 1.2742776547070949\n",
      "Cost after iterations 360000: 1.2742717232231082\n",
      "Cost after iterations 370000: 1.2742659226778055\n",
      "Cost after iterations 380000: 1.2742602501773652\n",
      "Cost after iterations 390000: 1.2742547028920959\n",
      "Cost after iterations 400000: 1.2742492780549743\n",
      "Cost after iterations 410000: 1.274243972960303\n",
      "Cost after iterations 420000: 1.2742387849622814\n",
      "Cost after iterations 430000: 1.2742337114737163\n",
      "Cost after iterations 440000: 1.2742287499647338\n",
      "Cost after iterations 450000: 1.2742238979614178\n",
      "Cost after iterations 460000: 1.2742191530446663\n",
      "Cost after iterations 470000: 1.2742145128489\n",
      "Cost after iterations 480000: 1.2742099750609266\n",
      "Cost after iterations 490000: 1.2742055374187384\n",
      "Cost after iterations 500000: 1.2742011977103618\n",
      "Cost after iterations 510000: 1.2741969537728228\n",
      "Cost after iterations 520000: 1.274192803490965\n",
      "Cost after iterations 530000: 1.2741887447964373\n",
      "Cost after iterations 540000: 1.274184775666651\n",
      "Cost after iterations 550000: 1.274180894123759\n",
      "Cost after iterations 560000: 1.274177098233647\n",
      "Cost after iterations 570000: 1.2741733861050015\n",
      "Cost after iterations 580000: 1.2741697558883243\n",
      "Cost after iterations 590000: 1.274166205775019\n",
      "Cost after iterations 600000: 1.274162733996468\n",
      "Cost after iterations 610000: 1.2741593388231724\n",
      "Cost after iterations 620000: 1.274156018563881\n",
      "Cost after iterations 630000: 1.2741527715647047\n",
      "Cost after iterations 640000: 1.2741495962083464\n",
      "Cost after iterations 650000: 1.2741464909132287\n",
      "Cost after iterations 660000: 1.2741434541327605\n",
      "Cost after iterations 670000: 1.2741404843544988\n",
      "Cost after iterations 680000: 1.2741375800994637\n",
      "Cost after iterations 690000: 1.2741347399213436\n",
      "Cost after iterations 700000: 1.2741319624057845\n",
      "Cost after iterations 710000: 1.2741292461696876\n",
      "Cost after iterations 720000: 1.2741265898605083\n",
      "Cost after iterations 730000: 1.2741239921555958\n",
      "Cost after iterations 740000: 1.274121451761527\n",
      "Cost after iterations 750000: 1.2741189674134406\n",
      "Cost after iterations 760000: 1.2741165378744106\n",
      "Cost after iterations 770000: 1.2741141619348562\n",
      "Cost after iterations 780000: 1.2741118384119052\n",
      "Cost after iterations 790000: 1.2741095661488013\n",
      "Cost after iterations 800000: 1.2741073440143538\n",
      "Cost after iterations 810000: 1.2741051709023437\n",
      "Cost after iterations 820000: 1.2741030457310032\n",
      "Cost after iterations 830000: 1.2741009674424275\n",
      "Cost after iterations 840000: 1.2740989350020913\n",
      "Cost after iterations 850000: 1.2740969473983066\n",
      "Cost after iterations 860000: 1.2740950036417429\n",
      "Cost after iterations 870000: 1.2740931027648852\n",
      "Cost after iterations 880000: 1.274091243821593\n",
      "Cost after iterations 890000: 1.2740894258866282\n",
      "Cost after iterations 900000: 1.274087648055165\n",
      "Cost after iterations 910000: 1.2740859094423649\n",
      "Cost after iterations 920000: 1.2740842091829274\n",
      "Cost after iterations 930000: 1.274082546430619\n",
      "Cost after iterations 940000: 1.2740809203579382\n",
      "Cost after iterations 950000: 1.2740793301556197\n",
      "Cost after iterations 960000: 1.274077775032287\n",
      "Cost after iterations 970000: 1.2740762542139907\n",
      "Cost after iterations 980000: 1.2740747669439048\n",
      "Cost after iterations 990000: 1.2740733124818908\n"
     ]
    }
   ],
   "source": [
    "parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.004, 1000000, parameters=parameters, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1: 0.4274763661178869\n",
      "0, 2: 0.2705866913023904\n",
      "0, 3: 0.4274763661178869\n",
      "0, 4: -0.09015680723840407\n",
      "0, 5: 0.2705866913023904\n",
      "0, 6: 0.4274763661178869\n",
      "0, 7: 0.4320663746882391\n",
      "1, 2: 0.2795420242392431\n",
      "1, 3: 1.0000000000000002\n",
      "1, 4: 0.35118295197677085\n",
      "1, 5: 0.27954202423924307\n",
      "1, 6: 1.0\n",
      "1, 7: 0.43450295360836166\n",
      "2, 3: 0.27954202423924296\n",
      "2, 4: -0.01701895476734841\n",
      "2, 5: 1.0000000000000002\n",
      "2, 6: 0.27954202423924307\n",
      "2, 7: 0.5477257188544188\n",
      "3, 4: 0.3511829519767708\n",
      "3, 5: 0.279542024239243\n",
      "3, 6: 0.9999999999999999\n",
      "3, 7: 0.4345029536083616\n",
      "4, 5: -0.017018954767348413\n",
      "4, 6: 0.3511829519767708\n",
      "4, 7: 0.1656207032022741\n",
      "5, 6: 0.27954202423924307\n",
      "5, 7: 0.5477257188544188\n",
      "6, 7: 0.4345029536083616\n"
     ]
    }
   ],
   "source": [
    "wrd_emb = parameters['WRD_EMB']\n",
    "\n",
    "for i in range(8):\n",
    "    for j in range(i + 1, 8):\n",
    "        vec_i, vec_j = wrd_emb[i, :], wrd_emb[j, :]\n",
    "        cos_sim = np.dot(vec_i, vec_j.T) / (np.linalg.norm(vec_i) * np.linalg.norm(vec_j))\n",
    "        print('{}, {}: {}'.format(i, j, cos_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28388168, -0.09018733, -0.1182425 , -0.6984883 , -0.34360891,\n",
       "         0.47949579,  0.02677859, -0.29275589,  0.13519994,  0.13533125,\n",
       "        -0.52338551, -0.32271853,  0.04203143, -0.07006129,  0.42923333],\n",
       "       [ 0.51724136, -0.35558248, -0.17801584, -0.62767221, -0.092183  ,\n",
       "         0.09996036,  0.18650278,  0.42500462, -0.20902071, -0.4160677 ,\n",
       "        -0.44719754,  0.32435583,  0.15416826,  0.06118605,  0.1125002 ],\n",
       "       [ 0.36451148,  0.16817632, -0.6901139 , -0.61564657, -0.07333198,\n",
       "        -0.14828185,  0.59707969,  0.45132495,  0.49851355,  0.15252522,\n",
       "         0.27534255, -0.19337081, -0.50989304,  0.39624888,  0.36182102],\n",
       "       [ 0.51724136, -0.35558248, -0.17801584, -0.62767221, -0.092183  ,\n",
       "         0.09996036,  0.18650278,  0.42500462, -0.20902071, -0.4160677 ,\n",
       "        -0.44719754,  0.32435583,  0.15416826,  0.06118605,  0.1125002 ],\n",
       "       [ 0.00631667, -0.72975983, -0.5577656 , -0.08515884, -0.23903792,\n",
       "         0.1744345 , -0.02969118, -0.21422726, -0.20975865, -0.45093506,\n",
       "         0.35531289,  0.60973753, -0.05776858,  0.03935581, -0.22674363],\n",
       "       [ 0.36451148,  0.16817632, -0.6901139 , -0.61564657, -0.07333198,\n",
       "        -0.14828185,  0.59707969,  0.45132495,  0.49851355,  0.15252522,\n",
       "         0.27534255, -0.19337081, -0.50989304,  0.39624888,  0.36182102],\n",
       "       [ 0.51724136, -0.35558248, -0.17801584, -0.62767221, -0.092183  ,\n",
       "         0.09996036,  0.18650278,  0.42500462, -0.20902071, -0.4160677 ,\n",
       "        -0.44719754,  0.32435583,  0.15416826,  0.06118605,  0.1125002 ],\n",
       "       [ 0.34994962, -0.02812211, -0.79215358, -0.11982547, -0.35394589,\n",
       "         0.21229515,  0.29660386,  0.22871262,  0.4218118 , -0.21769455,\n",
       "        -0.46155609, -0.25143938, -0.1061204 ,  0.03140572, -0.19439691]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrd_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
