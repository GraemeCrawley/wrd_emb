{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.1\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    W = np.random.randn(output_size, input_size) * 0.1\n",
    "    b = np.random.randn(output_size, 1) * 0.1\n",
    "    return W, b\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W, b = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    parameters['b'] = b\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds -- shape: (CBOW_N, number of examples)\n",
    "    \"\"\"\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vecs = np.take(WRD_EMB, inds, axis=0)\n",
    "    word_vecs = word_vecs.reshape(WRD_EMB.shape[1], inds.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs.shape == (WRD_EMB.shape[1], inds.shape[0], inds.shape[1]))\n",
    "    \n",
    "    return word_vecs\n",
    "\n",
    "def mean_(word_vecs):\n",
    "    word_vecs_mean = np.mean(word_vecs, axis=1)\n",
    "    word_vecs_mean = word_vecs_mean.reshape(word_vecs.shape[0], -1)\n",
    "    \n",
    "    assert(word_vecs_mean.shape == (word_vecs.shape[0], word_vecs.shape[2]))\n",
    "    \n",
    "    return word_vecs_mean\n",
    "\n",
    "def linear_dense(word_vecs_mean, parameters):\n",
    "    W, b = parameters['W'], parameters['b']\n",
    "    Z = np.dot(W, word_vecs_mean) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], word_vecs_mean.shape[1]))\n",
    "    \n",
    "    return W, b, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=1, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vecs = ind_to_word_vecs(inds, parameters)\n",
    "    word_vecs_mean = mean_(word_vecs)\n",
    "    W, b, Z = linear_dense(word_vecs_mean, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vecs'] = word_vecs\n",
    "    caches['word_vecs_mean'] = word_vecs_mean\n",
    "    caches['W'] = W\n",
    "    caches['b'] = b\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=1), axis=0)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, caches):\n",
    "    Z = caches['Z']\n",
    "    dL_dZ = Z - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == Z.shape)\n",
    "    \n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    W = caches['W']\n",
    "    b = caches['b']\n",
    "    word_vecs_mean = caches['word_vecs_mean']\n",
    "    m = word_vecs_mean.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vecs_mean.T)\n",
    "    dL_db = (1 / m) * np.mean(dL_dZ, axis=1, keepdims=True)\n",
    "    dL_dword_vecs_mean = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(b.shape == dL_db.shape)\n",
    "    assert(word_vecs_mean.shape == dL_dword_vecs_mean.shape)\n",
    "    \n",
    "    return dL_dW, dL_db, dL_dword_vecs_mean\n",
    "\n",
    "def mean_backward(dL_dword_vecs_mean, caches):\n",
    "    word_vecs = caches['word_vecs']\n",
    "    CBOW_N = word_vecs.shape[1]\n",
    "    \n",
    "    dL_dword_vecs = (1 / m) * (1 / CBOW_N) * np.ones((dL_dword_vecs_mean.shape[0], CBOW_N)) *\\\n",
    "        np.sum(dL_dword_vecs_mean, axis=1, keepdims=True)\n",
    "\n",
    "    assert((word_vecs.shape[0], word_vecs.shape[1]) == dL_dword_vecs.shape[:2])\n",
    "    \n",
    "    return dL_dword_vecs\n",
    "\n",
    "def backward_propagation(Y, caches):\n",
    "    dL_dZ = softmax_backward(Y, caches)\n",
    "    dL_dW, dL_db, dL_dword_vecs_mean = dense_backward(dL_dZ, caches)\n",
    "    dL_dword_vecs = mean_backward(dL_dword_vecs_mean, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_db'] = dL_db\n",
    "    gradients['dL_dword_vecs'] = dL_dword_vecs\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    CBOW_N = caches['inds'].shape[0]\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    \n",
    "    inds = caches['inds']\n",
    "    updated_WRD_EMD = parameters['WRD_EMB'][inds.T, :] -\\\n",
    "        learning_rate * gradients['dL_dword_vecs'].T.reshape(1, CBOW_N, -1)\n",
    "    parameters['WRD_EMB'][inds.flatten(), :] = updated_WRD_EMD.reshape(-1, emb_size)\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']\n",
    "    parameters['b'] -= learning_rate * gradients['dL_db']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "\n",
    "    batch_inds = list(range(0, m, batch_size))\n",
    "    for epoch in tqdm_notebook(range(epochs)):\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in tqdm_notebook(batch_inds[:1000]):\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "\n",
    "        cost = cross_entropy(softmax_out, Y_batch)\n",
    "        costs.append(cost)\n",
    "        if print_cost and epoch % 25 == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, np.squeeze(cost)))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy data\n",
    "Sentence: I(0) would(1) like(2) to(3) get(4) a(5) better(6) job(7).  \n",
    "vocab_size = 8  \n",
    "```\n",
    "[0, 2] [1]  \n",
    "[1, 3] [2]  \n",
    "[2, 4] [3]  \n",
    "[3, 5] [4]  \n",
    "[4, 6] [5]  \n",
    "[5, 7] [6]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_len = 2\n",
    "# vocab_size = 8\n",
    "# m = 6\n",
    "# emb_size = 15\n",
    "# X = np.array([[0, 1, 2, 3, 4, 5],\n",
    "#               [2, 3, 4, 5, 6, 7]]) # 2 x 6\n",
    "# Y = np.array([1, 2, 3, 4, 5, 6]) # 1 x 6\n",
    "# Y_one_hot = np.zeros((vocab_size, m))  # 8 x 6\n",
    "# Y_one_hot[Y.flatten(), np.arange(6)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = initialize_parameters(vocab_size, emb_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_out, caches = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost = cross_entropy(softmax_out, Y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Probagation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradients = backward_propagation(Y_one_hot, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.005, 1000000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.003, 1000000, parameters=parameters, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.004, 1000000, parameters=parameters, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Overflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickles/X.pkl', 'rb') as file:\n",
    "    X = pickle.load(file)\n",
    "    \n",
    "with open('pickles/Y.pkl', 'rb') as file:\n",
    "    Y = pickle.load(file)\n",
    "    \n",
    "assert(X.shape[-1] == Y.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 31853\n",
    "m = X.shape[-1]\n",
    "batch_size = 256\n",
    "emb_size = 50\n",
    "\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5347d75b8bf439e9cdebcd5a764fd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad652392130422593686282727cbc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = cbow_model(X, Y_one_hot, vocab_size, emb_size, 0.005, 2000, batch_size=128, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRD_EMB = parameters['WRD_EMB']\n",
    "\n",
    "# with open('pickles/word_to_id.pkl', 'rb') as file:\n",
    "#     word_to_id = pickle.load(file)\n",
    "    \n",
    "# with open('pickles/id_to_word.pkl', 'rb') as file:\n",
    "#     id_to_word = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar(word, wrd_emb, n=10):\n",
    "    id_ = word_to_id[word]\n",
    "    vec_word = wrd_emb[id_, :]\n",
    "    norm_vec_word = np.linalg.norm(vec_word)\n",
    "    cos_sim = np.dot(wrd_emb, vec_word.T) / (np.linalg.norm(wrd_emb, axis=1) * norm_vec_word)\n",
    "    top_n_ind = np.argsort(cos_sim)[-n:][::-1]\n",
    "    return top_n_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = find_top_n_similar('sort', WRD_EMB, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id_to_word[id_] for id_ in inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[156]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id['bfs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
